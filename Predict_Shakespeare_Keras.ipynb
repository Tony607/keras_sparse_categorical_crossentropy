{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example two - character level sequence to sequence prediction\n",
    "Tutorial [How to use Keras sparse_categorical_crossentropy](https://www.dlology.com/blog/how-to-use-keras-sparse_categorical_crossentropy/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rxkNIKFM1vWx"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edfbxDDh2AEs"
   },
   "source": [
    "# Predict Shakespeare with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xzpUtDMqmA-x"
   },
   "source": [
    "This example uses [tf.keras](https://www.tensorflow.org/guide/keras) to build a *language model* and train it on a GPU. This language model predicts the next character of text given the text so far. The trained model can generate new snippets of text that read in a similar style to the text training data.\n",
    "\n",
    "We'll train the model on the combined works of William Shakespeare, then use it to compose a play in the style of *The Great Bard*:\n",
    "\n",
    "<blockquote>\n",
    "Loves that led me no dumbs lack her Berjoy's face with her to-day.  \n",
    "The spirits roar'd; which shames which within his powers  \n",
    "\tWhich tied up remedies lending with occasion,  \n",
    "A loud and Lancaster, stabb'd in me  \n",
    "\tUpon my sword for ever: 'Agripo'er, his days let me free.  \n",
    "\tStop it of that word, be so: at Lear,  \n",
    "\tWhen I did profess the hour-stranger for my life,  \n",
    "\tWhen I did sink to be cried how for aught;  \n",
    "\tSome beds which seeks chaste senses prove burning;  \n",
    "But he perforces seen in her eyes so fast;  \n",
    "And _  \n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KRQ6Fjra3Ruq"
   },
   "source": [
    "## Download data\n",
    "\n",
    "Download *The Complete Works of William Shakespeare* as a single text file from [Project Gutenberg](https://www.gutenberg.org/). We'll use snippets from this file as the *training data* for the model. The *target* snippet is offset by one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "j8sIXh1DEDDd",
    "outputId": "e4850c73-b024-4845-a425-2c911515b8a5"
   },
   "outputs": [],
   "source": [
    "# !wget --show-progress --continue -O data/shakespeare.txt http://www.gutenberg.org/files/100/100-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbL6cqCl7hnt"
   },
   "source": [
    "### Build the data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "E3V4V-Jxmuv3",
    "outputId": "bba5a040-6d8d-41fa-e20b-3118531d75cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hasee\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Input text [5834393] ﻿\n",
      "Project Gutenberg’s The Complete Works of Willi\n",
      "x, shape: (2, 10) , x: [[109 109 117 110 105  99  97 116  39 115]\n",
      " [114 101  32  97 110 103 114 121  13  10]]\n",
      "y, shape: (2, 10, 1) , y: [[[109]\n",
      "  [117]\n",
      "  [110]\n",
      "  [105]\n",
      "  [ 99]\n",
      "  [ 97]\n",
      "  [116]\n",
      "  [ 39]\n",
      "  [115]\n",
      "  [116]]\n",
      "\n",
      " [[101]\n",
      "  [ 32]\n",
      "  [ 97]\n",
      "  [110]\n",
      "  [103]\n",
      "  [114]\n",
      "  [121]\n",
      "  [ 13]\n",
      "  [ 10]\n",
      "  [ 32]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "SHAKESPEARE_TXT = 'data/shakespeare.txt'\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "def transform(txt, pad_to=None):\n",
    "    \"\"\"\n",
    "    Transform chars in txt to ascii values np.array, drop non-ascii chars.\n",
    "    \"\"\"\n",
    "    # drop any non-ascii characters\n",
    "    output = np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
    "    if pad_to is not None:\n",
    "        output = output[:pad_to]\n",
    "        output = np.concatenate([\n",
    "            np.zeros([pad_to - len(txt)], dtype=np.int32),\n",
    "            output,\n",
    "        ])\n",
    "    return output\n",
    "\n",
    "def training_generator(seq_len=100, batch_size=1024):\n",
    "    \"\"\"A generator yields (source, target) arrays for training.\"\"\"\n",
    "    with tf.gfile.GFile(SHAKESPEARE_TXT, 'r') as f:\n",
    "        txt = f.read()\n",
    "\n",
    "    tf.logging.info('Input text [%d] %s', len(txt), txt[:50])\n",
    "    source = transform(txt)\n",
    "    while True:\n",
    "        # One batch of offsets for sampling sequences randomly.\n",
    "        offsets = np.random.randint(low=0, high=len(source) - seq_len, size=batch_size)\n",
    "\n",
    "        # Our model uses sparse crossentropy loss, but Keras requires labels\n",
    "        # to have the same rank as the input logits.  We add an empty final\n",
    "        # dimension to account for this.\n",
    "        yield (\n",
    "            np.stack([source[idx:idx + seq_len] for idx in offsets]),\n",
    "            np.expand_dims(\n",
    "                np.stack([source[idx + 1:idx + seq_len + 1] for idx in offsets]),\n",
    "                -1),\n",
    "        )\n",
    "\n",
    "x, y = six.next(training_generator(seq_len=10, batch_size=2))\n",
    "print('x, shape:', x.shape, ', x:', x)\n",
    "print('y, shape:', y.shape, ', y:', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49, 50, 51, 32, 98, 99, 32, 65, 66, 67])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform('123 bc ABC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bbb05dNynDrQ"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "The model is defined as a two-layer, forward-LSTM—with two changes from the `tf.keras` standard LSTM definition:\n",
    "\n",
    "1. Define the input `shape` of our model which satisfies the [XLA compiler](https://www.tensorflow.org/performance/xla/)'s static shape requirement.\n",
    "2. Use `tf.train.Optimizer` instead of a standard Keras optimizer (Keras optimizer support is still experimental).\n",
    "\n",
    "By using `sparse_categorical_crossentropy` we can have keep integer as chars' labels without transforming to one-hot labels.\n",
    "So the output of the model will be in softmaxed one-hot like shape while the labels are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLEM-fLJlEEt"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "MAX_TOKENS = 256\n",
    "def lstm_model(seq_len=100, batch_size=None, stateful=True, max_tokens = 256):\n",
    "    \"\"\"Language model: predict the next char given the current char.\"\"\"\n",
    "    source = tf.keras.Input(\n",
    "        name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
    "\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=max_tokens, output_dim=EMBEDDING_DIM)(source)\n",
    "    lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
    "    lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
    "    predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(max_tokens, activation='softmax'))(lstm_2)\n",
    "    model = tf.keras.Model(inputs=[source], outputs=[predicted_char])\n",
    "    model.compile(\n",
    "        optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VzBYDJI0_Tfm"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "The `tf.contrib.tpu.keras_to_tpu_model` function converts a `tf.keras` model to an equivalent TPU version. We then use the standard Keras methods to train: `fit`, `predict`, and `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 910
    },
    "colab_type": "code",
    "id": "ExQ922tfzSGA",
    "outputId": "fa035593-38eb-4e21-d050-c1ee95528af0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "seed (InputLayer)            (128, 100)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (128, 100, 512)           131072    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (128, 100, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (128, 100, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (128, 100, 256)           131328    \n",
      "=================================================================\n",
      "Total params: 4,460,800\n",
      "Trainable params: 4,460,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "INFO:tensorflow:Input text [5834393] ﻿\n",
      "Project Gutenberg’s The Complete Works of Willi\n",
      "100/100 [==============================] - 65s 652ms/step - loss: 3.9236 - sparse_categorical_accuracy: 0.1840\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 57s 571ms/step - loss: 3.2356 - sparse_categorical_accuracy: 0.2051\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "training_model = lstm_model(seq_len=100, batch_size=128, stateful=False, max_tokens = MAX_TOKENS)\n",
    "\n",
    "# tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
    "#     training_model,\n",
    "#     strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "#         tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
    "\n",
    "training_model.summary()\n",
    "\n",
    "training_model.fit_generator(\n",
    "    training_generator(seq_len=100, batch_size=128),\n",
    "    steps_per_epoch=100,\n",
    "    epochs=2, # 10\n",
    ")\n",
    "training_model.save_weights('models/bard-GPU-epoch2.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCBtcpZkykSf"
   },
   "source": [
    "## Make predictions with the model\n",
    "\n",
    "Use the trained model to make predictions and generate your own Shakespeare-esque play.\n",
    "Start the model off with a *seed* sentence, then generate 250 characters from it. We'll make five predictions from the initial seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "seed (InputLayer)            (5, 1)                    0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (5, 1, 512)               131072    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (5, 1, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (5, 1, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (5, 1, 256)               131328    \n",
      "=================================================================\n",
      "Total params: 4,460,800\n",
      "Trainable params: 4,460,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5\n",
    "PREDICT_LEN = 250\n",
    "\n",
    "# Keras requires the batch size be specified ahead of time for stateful models.\n",
    "# We use a sequence length of 1, as we will be feeding in one character at a \n",
    "# time and predicting the next character.\n",
    "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True, max_tokens = MAX_TOKENS)\n",
    "prediction_model.load_weights('models/bard-GPU-epoch2.h5') # bard-TPU-epoch10.h5 or bard-GPU-epoch2.h5\n",
    "\n",
    "# We seed the model with our initial string, copied BATCH_SIZE times\n",
    "\n",
    "seed_txt = 'Looks it not like the king?  Verily, we must go! '\n",
    "\n",
    "# Text chars to ascii values np.array.\n",
    "seed = transform(seed_txt)\n",
    "\n",
    "# Repeat seed for batch size times.\n",
    "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
    "\n",
    "prediction_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1241
    },
    "colab_type": "code",
    "id": "tU7M-EGGxR3E",
    "outputId": "18dce114-3500-4258-e628-e6326400fe1f"
   },
   "outputs": [],
   "source": [
    "# First, run the seed forward to prime the state of the model.\n",
    "prediction_model.reset_states()\n",
    "\n",
    "# Kick start with the seed text.\n",
    "for i in range(len(seed_txt) - 1):\n",
    "    prediction_model.predict(seed[:, i:i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [seed[:, -1:]]\n",
    "predictions[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last chars in the batch\n",
    "last_char = predictions[-1]\n",
    "# Predict with only the last chars as input\n",
    "next_probits = prediction_model.predict(last_char)[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1401cd91860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(next_probits[1])\n",
    "plt.xlabel('ascii')\n",
    "plt.ylabel('probs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can accumulate predictions!\n",
    "\n",
    "# The last char's ascii value for each sequence in seed batch (5 sequences).\n",
    "predictions = [seed[:, -1]]\n",
    "\n",
    "# Predict PREDICT_LEN(250) in number of chars.\n",
    "for i in range(PREDICT_LEN):\n",
    "    # Last chars in the batch\n",
    "    last_char = predictions[-1]\n",
    "    \n",
    "    # Predict with only the last chars as inputs.\n",
    "    next_probits = prediction_model.predict(last_char)[:, 0, :]\n",
    "\n",
    "    # Sample from output distribution for each sample in the batch.\n",
    "    next_idx = [\n",
    "        np.random.choice(len(next_probits[0]), p=next_probits[i])\n",
    "        for i in range(BATCH_SIZE)\n",
    "    ]\n",
    "    # Collect the sampled output char ascii values.\n",
    "    predictions.append(np.asarray(next_idx, dtype=np.int32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new scripts\n",
    "For better performance, either train for more epochs or download the pre-trained weigths [bard-TPU-epoch10.h5](https://github.com/Tony607/keras_sparse_categorical_crossentropy/releases/download/V0.1/bard-TPU-epoch10.h5) and copy over to `./models` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION 0\n",
      "\n",
      "\n",
      "       Exit\r\n",
      "  PETER. Most fly with her?\r\n",
      "\r\n",
      "                    Re-enter the Colcertal\r\n",
      "  SALERIO, Servants, countenance and Bon to Goneril.\r\n",
      "  SHEPHERD. So should I have forsworn: and follow him dowrier, and he\r\n",
      "    Life against whom thy last consen\n",
      "\n",
      "PREDICTION 1\n",
      "\n",
      "\n",
      "      [Flourish] Enter, one\r\n",
      "  FORD. No, neither?\r\n",
      "  CONSPERS. Madam, and in me, call I warrant you!\r\n",
      "  PROSPERO. Fesperas, you note; et a foul King;\r\n",
      "    And from the sun nor the excellent good.\r\n",
      "    Your fair dotages in arms;\r\n",
      "    For my verses, wh\n",
      "\n",
      "PREDICTION 2\n",
      "\n",
      "\n",
      "        Exeunt COMERIUS\r\n",
      "    He is gordeity?\r\n",
      "  MICHAEL. I dare not torture, measure.\r\n",
      "  VIOLA. Rerlain, how was not the fiend?\r\n",
      "  SOMERSET. Give me the humours, coming for this:\r\n",
      "Ha! Would I send love memorizo? when one one farewele to see below,\r\n",
      "H\n",
      "\n",
      "PREDICTION 3\n",
      "\n",
      "\n",
      "     552\r\n",
      "\r\n",
      "As one more but that from him-followers,\r\n",
      "Ohlelly consents and vows of all\r\n",
      "enemies love as you know me.\r\n",
      "\r\n",
      "PRINCE.\r\n",
      "Not what you are sick; unto the people, and good heart\r\n",
      "in men, do well example. Were nature to what, son,\r\n",
      "Did not now b\n",
      "\n",
      "PREDICTION 4\n",
      "\n",
      "\n",
      "                  \r\n",
      "    Sens all, good monster, there's compared by your friend,\r\n",
      "            That is very wretches than Sicining my Lords of Goths,\r\n",
      "               And in a Christians\r\n",
      "            Tripping souls and trusting willingly.\r\n",
      "  DUCHESS. D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each batch, ascii values -> chars.\n",
    "for i in range(BATCH_SIZE):\n",
    "    print('PREDICTION %d\\n\\n' % i)\n",
    "    p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
    "    generated = ''.join([chr(c) for c in p])\n",
    "    print(generated)\n",
    "    print()\n",
    "    assert len(generated) == PREDICT_LEN, 'Generated text too short'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYp6XVQU2POn"
   },
   "source": [
    "### To run the code with TPU on Colab\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    " <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Predict Shakespeare with Cloud TPUs and Keras",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
